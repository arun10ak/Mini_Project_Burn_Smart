# -*- coding: utf-8 -*-
"""Final Mini Project Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fATkdVFBSWUqXr9jYbAvoCxWEyCJPypR

**1.Importing the Dependencies**
"""

#import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder,StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestRegressor


from sklearn import metrics
from sklearn.metrics import mean_absolute_error as mae
from sklearn import set_config

set_config(display='diagram')
from google.colab import drive
drive.mount('/content/drive')

import pickle

"""**1.Data Collection & Processing**

***1.Load the dataset***
"""

#Load the dataset
dataset = pd.read_csv('/content/drive/MyDrive/MiniProject -SRM Dataset/Exercise and Calories Burn.csv')

"""***2.Summarize the dataset***"""

#shape of the dataset
print("Dataset Shape:",dataset.shape)
print("***"*20)

#checking null values is present or not
print("The dataset have the null values or not:\n",dataset.isnull().sum())
print("***"*20)

#check the unique values
print("The unique values present in each of the column as follows:")
print(dataset.nunique(),"\n")
print("***"*20)

#information of data
print("The information of data followed below:")
print(dataset.info())
print("***"*20)

#get some statistical measures about the data
print("The statistical measures about the dataset as follows:")
print(dataset.describe())
print("***"*20)

#view
print("The first 5 set of rows from the dataset shown below:")
print(dataset.head())
print("***"*20)

#Drop the unanted feature from the dataset
data = dataset
data = data.drop(columns=['User_ID'], axis=1)
print("Data Shape:",data.shape)
print(data.head())

"""***3.Data Visualization and Distribution***

==> ***Count Plot***
"""

#count of gender in the dataset
sns.set()
plt.figure(figsize=(6,6))
ax=sns.countplot(x=data.Gender,palette = "Set1")
ax.bar_label(ax.containers[0], fmt='{:.0f}')
plt.title("Count of Gender")
plt.show()

"""==> **Pie Chart**"""

#pie chart of gender in %
plt.figure(figsize=(5,7))
plt.pie(data.Gender.value_counts(),labels=data.Gender.value_counts().index,autopct="%.3f%%",shadow=True)
plt.title("% of Gender")
plt.legend()
plt.show()

"""==> **Pair Plot**"""

#pairplot
sns.set()
plt.figure(figsize=(6,6))
sns.pairplot(data, hue='Calories_Burn', height=2)
plt.show()

"""==> **Distribution Plot**"""

#Distribution of Age
plt.figure(figsize=(6,6))
sns.distplot(x=data.Age,color='blue')
plt.title("Distribution of Age")
plt.show()

#Distribution of Height
plt.figure(figsize=(6,6))
sns.distplot(x=data.Height,color='blue')
plt.title("Distribution of Height")
plt.show()

#Distribution of Weight
plt.figure(figsize=(6,6))
sns.distplot(x=data.Weight,color='blue')
plt.title("Distribution of Weight")
plt.show()

#Distribution of Duration
plt.figure(figsize=(6,6))
sns.distplot(x=data.Duration,color='blue')
plt.title("Distribution of Duration")
plt.show()

#Distribution of Heart Rate
plt.figure(figsize=(6,6))
sns.distplot(x=data.Heart_Rate,color='blue')
plt.title("Distribution of Heart Rate")
plt.show()

#Distribution of Body Temperature
plt.figure(figsize=(6,6))
sns.distplot(x=data.Body_Temp,color='blue')
plt.title("Distribution of Body Temperature")
plt.show()

#Distribution of Calories Burn
plt.figure(figsize=(6,6))
sns.distplot(x=data.Calories_Burn,color='blue')
plt.title("Distribution of Calories Burn")
plt.show()

"""The distribution of the continuous features follows
close to normal distribution except for some features like Body_Temp and Calories.

==> **Heat Map**
"""

#correlation between the each column of the dataset
print("Correlation among the columns follows below:")
print(data.corr())

#heatmap
plt.figure(figsize=(10,10))
sns.heatmap(data.corr(),annot=True)
plt.title("Correlation between Columns")

#corr value round to -1 to 1
#heatmap
correlation = data.corr()
plt.figure(figsize=(10,10))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':8}, cmap='Reds')

"""==> **Scatter Plot**"""

#Relation between Age and Calories Burn
plt.figure(figsize=(6,6))
sns.scatterplot(x=data.Age,y=data.Calories_Burn,color='green')
plt.title("Relation between Age and Calories Burn")
plt.show()

#Relation between Height and Calories Burn
plt.figure(figsize=(6,6))
sns.scatterplot(x=data.Height,y=data.Calories_Burn,color='green')
plt.title("Relation between Height and Calories Burn")
plt.show()

#Relation between Weight and Calories Burn
plt.figure(figsize=(6,6))
sns.scatterplot(x=data.Weight,y=data.Calories_Burn,color='green')
plt.title("Relation between Weight and Calories Burn")
plt.show()

#Relation between Duration and Calories Burn
plt.figure(figsize=(6,6))
sns.scatterplot(x=data.Duration,y=data.Calories_Burn,color='green')
plt.title("Relation between Duration and Calories Burn")
plt.show()

"""*   As expected higher is the duration of the workout higher will be the calories burnt.But except for that, we cannot observe any such relation between calories burnt and height or weight features.

*   The average height of the boys is higher than girls.Also, the weight of the girls is lower than that of the boys.

*  For the same average duration of workout calories burnt by men is higher than that of women.
"""

#Relation between Height and Weight
plt.figure(figsize=(6,6))
sns.scatterplot(x=data.Height,y=data.Weight)
plt.title("Relation between Height and Weight")
plt.show()

"""So, we have a kind of linear relationship between these two features which is quite obvious.

**4.Model Selection**

**Label Encoding**
"""

#encoding
lb=LabelEncoder()
data['Gender']=lb.fit_transform(data['Gender'])
print(lb.classes_)
print("Gender:")
print("      Female --> 0 \n      Male   --> 1")
print("***"*20)
print(data.head())
print("***"*20)

"""**Splitting Input and Output features from the dataset**"""

#split the X and y values
X=data.iloc[:,:-1]
y=data.iloc[:,-1]
print("Input Features of the data:")
print(X)
print("***"*25)
print("Output Feature of the data:")
print(y)
print("***"*25)

"""**Splitting the X & y  into training data and Testing data**"""

#split train and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Input Shape:",X.shape)
print("Training Set Shape:",X_train.shape)
print("Testing Set Shape:",X_test.shape)
print("***"*25)

"""**Rescale the features of X**"""

#rescale feature
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Different Algorithms is used for this  and choose the best model which as lowest mae

**5.Evaluate the model**
"""

from sklearn.metrics import mean_absolute_error as mae
models = [LinearRegression(), Ridge(), Lasso(), RandomForestRegressor(), XGBRegressor() ]

for i in range(5):
    models[i].fit(X_train, y_train)

    print(f'{models[i]} : ')

    train_preds = models[i].predict(X_train)
    print('Training Error : ', mae(y_train, train_preds))

    y_pred = models[i].predict(X_test)
    print('Validation Error : ', mae(y_test, y_pred))
    print()

"""* Out of all the above models, we have trained RandomForestRegressor and
the XGB modelâ€™s performance is the same as their MAE for the validation data is same.

* The XGBRegressor model has a lower validation error than the RandomForestRegressor model, which suggests that it is less likely to overfit the data and perform better on unseen data. Therefore, the XGBRegressor model is the better choice for deployment.

**Optimisation**

*For Ridge Regression*
"""

#initialising Ridge() function
ridge = Ridge()
params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,
                    1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0,10.0, 20, 50, 100, 500, 1000 ]}

# defining cross validation folds as 5
folds =5
grid_cv_model = GridSearchCV(estimator=ridge,
                       param_grid=params,
                       scoring='neg_mean_absolute_error',
                       cv=folds,
                       return_train_score=True,
                       verbose=1)

# fiting GridSearchCV() with X_train and y_train
l2 = grid_cv_model.fit(X_train,y_train)
print("Best score for Ridge :",l2.best_score_)
print("The best alpha for Ridge is:",l2.best_params_)

"""*For Lasso Regression*"""

#initialising lasso() function
lasso = Lasso()
params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
                    0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0,
                    10.0, 20, 50, 100, 500, 1000 ]}

# defining cross validation folds as 5
folds =5
grid_cv_model = GridSearchCV(estimator=lasso,
                       param_grid=params,
                       scoring='neg_mean_absolute_error',
                       cv=folds,
                       return_train_score=True,
                       verbose=1)

# fiting GridSearchCV() with X_train and y_train
l1 = grid_cv_model.fit(X_train,y_train)
print("Best score for Lasso:",l1.best_score_)
print("The best alpha for Lasso is:",l1.best_params_)

"""*For Random Forest Regressor*"""

rfr = RandomForestRegressor(random_state=0)
parameters = {'n_estimators': [100, 150, 200, 250, 300],'max_depth': [1,2,3,4],}
grid_cv_model = GridSearchCV(rfr, parameters)

rfr_model = grid_cv_model.fit(X_train, y_train)
print("Best score for RandomForestRegressor",rfr_model.best_score_)
print("The best alpha for RandomForestRegressor is:",rfr_model.best_params_)

"""*For XGB Regressor*"""

xgb = XGBRegressor()
parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}

grid_cv_model = GridSearchCV(xgb,
                        parameters,
                        cv = 2,
                        n_jobs = 5,
                        verbose=True)

xgb_model = grid_cv_model.fit(X_train, y_train)
print("Best score for RandomForestRegressor",xgb_model.best_score_)
print("The best alpha for RandomForestRegressor is:",xgb_model.best_params_)

"""*Validate the model*"""

from sklearn.metrics import mean_absolute_error as mae
models = [LinearRegression(), Ridge(alpha=10), Lasso(alpha=0.05),RandomForestRegressor(),
          XGBRegressor(colsample_bytree=0.7,learning_rate=0.07,max_depth=5,min_child_weight=4,
                       n_estimators=500,nthread=4,objective='reg:linear',subsample=0.7)]

for i in range(5):
    models[i].fit(X_train, y_train)

    print(f'{models[i]} : ')

    train_preds = models[i].predict(X_train)
    print('Training Error : ', mae(y_train, train_preds))

    y_pred = models[i].predict(X_test)
    print('Validation Error : ', mae(y_test, y_pred))
    print()

"""**Best Model**"""

#bestmodel
xgb = XGBRegressor(colsample_bytree=0.7,learning_rate=0.07,max_depth=5,min_child_weight=4,
                   n_estimators=500,nthread=4,objective='reg:linear',subsample=0.7)
model = xgb.fit(X_train, y_train)

"""**To predict the unseen data**"""

#unseen data
print("Chosse Your Gender according to the Refrence Given Below.")
print("Female --> 0\nMale   --> 1")
gender = int(input("Enter Your Gender: "))
print('*'*60)

print("Choose The Age From 17.")
age = int(input("Enter Your Age: "))
print('*'*60)

height = float(input("Enter Your Height (in cm) : "))
print('*'*60)

weight = float(input("Enter Your Weight (in kg) : "))
print('*'*60)

duration = int(input("Enter Your duration (in mins) : "))
print('*'*60)

heart_rate = int(input("Enter Your Heart Rate from 60 to 130 : " ))
print('*'*60)

body_temp = int(input("Enter Your Body temp from 36 to 42 (in celsius ) : " ))
print('*'*60)

#predict calories burn
new_entry = [[gender,age,height,weight,duration,heart_rate,body_temp]]
Predict_lr = model.predict(new_entry)
print("Calories burn by Your above details:",Predict_lr)

"""**6.Deploy the model**"""

#deploy model
filename = "best_model_cb_pred.sav"
pickle.dump(xgb, open(filename, 'wb'))

"""steps follow to implement the code using streamlit library to create a wep page

*   From the left side of colab click file icon on this Download the "best_model_cb_pred.sav".
*   Then implementation will be in the spider concle to create a webpage using the above downloaded file.

**-------------------------------------------------------------------------------END-------------------------------------------------------------------------------**
"""

import numpy as np
import streamlit as st
import pickle

loaded_model = pickle.load(open('C:/Users/arunk/Desktop/Final Mini/best_model_cb_pred.sav','rb'))


def prediction(new_entry):
    Predict_lr = loaded_model.predict(new_entry)
    outp = float(Predict_lr)
    return (outp)

def main():
    st.title("Calories burn prediction")
    gender = st.number_input("Enter Your Gender as (Female:0,Male:1) ",step=0,min_value=0, max_value=1)
    age = st.number_input("Enter Your Age: ",step=0,min_value=15, max_value=100)
    height = st.number_input("Enter Your Height (in cm) : ",step=0,min_value=100, max_value=200)
    weight = st.number_input("Enter Your Weight (in kg) : ",step=0,min_value=40, max_value=200)
    duration = st.number_input("Enter Your duration (in mins) : ",step=0,min_value=1, max_value=180)
    heart_rate = st.number_input("Enter Your Heart Rate from 60 to 130 : ",step=0,min_value=60, max_value=130 )
    body_temp = st.number_input("Enter Your Body temp from 36 to 42 (in celsius ) : ",step=0,min_value=36, max_value=42 )


    new_entry = [[gender,age,height,weight,duration,heart_rate,body_temp]]
    # ctreating a button for prediction
    if st.button('Calories burned'):
        predicted_lr = prediction(new_entry)
        st.write("Calories burned:")
        st.success(predicted_lr)

if __name__=='__main__':
    main()